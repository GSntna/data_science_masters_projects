{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gustavo Santana Velázquez\n",
    "\n",
    "# Introducción\n",
    "\n",
    "En lingüistica, la entropía es una medida que calcula la complejidad y la impredicibilidad\n",
    "de un texto. Se basa en la probabilidad de ocurrencia de cada carácter o palabra en\n",
    "un texto y se utiliza para evaluar cuánta información se puede obtener del texto.\n",
    "\n",
    "Cuanto mayor sea la entropía, mayor será la complejidad e impredicibilidad del texto,\n",
    "es decir, hay una mayor variedad de palabras o caracteres utilizados y su distribución\n",
    "es más uniforme. Por el otro lado, una entroía baja indica que el texto es más\n",
    "predecible y hay una menor variedad de palabras o caracteres utilizadas con una distribución\n",
    "más concentrada.\n",
    "\n",
    "En resumen, la entropía se puede interpretar como una medida de la riqueza léxica\n",
    "de un texto, y puede utilizarse para comparar la complejidad y el estilo de diferentes\n",
    "textos.\n",
    "\n",
    "En este trabajo se evaluará la entropía de 5 textos cortos: Cantata a Satanás, el\n",
    "hereje rebelde, mimí sin bikini, los locos somos otro cosmos y un gurú vudú; cada\n",
    "uno tiene la particularidad de utilizar únicamente una vocal, por lo que se espera\n",
    "observar una entropía baja. Posteriormente se calculará la entropía de 2 libros:\n",
    "Niebla, de Miguel Unamono; y Marianela, por Benito Pérez Galdós. Para estos últimos\n",
    "se calculará la entropía con y sin *stopwords* para determinar qué tanto impacto\n",
    "tienen estas en el resultado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo\n",
    "\n",
    "La entropía $H(X)$ de un texto se puede obtener a través de la siguiente fórmula:\n",
    "$$H(X) = - \\sum p(x) \\log_{2}p(x)$$\n",
    "donde $p(x)$ la función de probabilidad de una variable aleatoria $X$ sobre un\n",
    "conjunto discreto de símbolos (o alfabeto).\n",
    "\n",
    "Los textos y libros fueron procesados antes de calcular la entropía, removiendo\n",
    "acentos, símbolos y números (Apéndice 5.2). Se definió una función en *Python* para\n",
    "calcular la entropía a nivel carácter o palabra para se utilziada en este ejercicio\n",
    "(Apéndice 4.3).\n",
    "\n",
    "**Para los textos 1-5 se obtuvieron los siguientes resultados:** (Apéndice 5.4)\n",
    "\n",
    "| Texto | Entropía | Número de caracteres | Repetición de vocal | % de repetición |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| text_1 | 3.080115476 | 4,267 | 1,863 | 43.66% |\n",
    "| text_2 | 3.087512876 | 3,227 | 1,360 | 42.14% |\n",
    "| text_3 | 3.265331452 | 1,326 | 485 | 36.58% |\n",
    "| text_4 | 3.116194809 | 2,946 | 1,179 | 40.02% |\n",
    "| text_5 | 3.171419752 | 1,340 | 557 | 41.57% |\n",
    "\n",
    "Se puede observar que la entropía es inversamente proporcional al % de repetición\n",
    "de la vocal en el texto. Esto es debido a que es la misma vocal la que se repite,\n",
    "la variedad de carácteres es menor y el texto es predecible, es por esto que en los\n",
    "textos en los que se repite la vocal más veces, la entropía disminuye.\n",
    "\n",
    "**En los libros 1 y 2 con stopwords** (Apéndice 5.5.1)\n",
    "\n",
    "| Libro | Entropía | Número de palabras |\n",
    "| --- | --- | --- |\n",
    "| libro_1 | 9.228294320935682 | 56,810 |\n",
    "| libro_2 | 9.611731124522716 | 50,536 |\n",
    "\n",
    "**Entropía de los libros 1 y 2 sin stopwords** (Apéndice 5.5.2)\n",
    "\n",
    "| Libro | Entropía | Número de palabras |\n",
    "| --- | --- | --- |\n",
    "| libro_1 | 11.353860554400953 | 26,307 |\n",
    "| libro_2 | 11.723176507369077 | 25,170 |\n",
    "\n",
    "La entropía en los libros resulta mucho mayor que en los 5 textos pasados, esto\n",
    "se debe a la incorporación de más variaciones (palabras vs carácteres del alfabeto)\n",
    "además de menos repeticiones.\n",
    "\n",
    "La entropía de los libros es muy similar, aunque la del libro 2 es un poco más grande,\n",
    "los que indica que hay una probabilidad de que sea más complejo.\n",
    "\n",
    "Tras remover las stopwords (que cabe mencionar que usualmente son las palabras más\n",
    "repetidas en un texto), la entropía aumenta en apróximadamente 2.1 para ambos libros.\n",
    "Por lo que se puede apreciar la influencia que estas tienen en el cálculo de esta\n",
    "medida. Tabién se observa a partir de las tablas que las stopwords componen\n",
    "aproximadamente la mitad del total de palabras para cada uno de los libros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "En conclusión, se realizó un análisis de entropía para un conjunto de textos\n",
    "cortos y dos libros. Se encontró que los libros tenían una entropía más alta que\n",
    "los textos cortos, lo que sugiere una mayor variedad léxica en el lenguaje utilizado\n",
    "en los libros. Además, se descubrió que al eliminar las stopwords, la entropía aumentó\n",
    "en ambos libros, lo que indica que las stopwords contribuyen significativamente\n",
    "a la repetición de palabras en los textos. En general, la entropía es una medida\n",
    "útil para cuantificar la diversidad léxica de un texto y puede ser útil para comprender\n",
    "las características del lenguaje utilizado en diferentes contextos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "[Jurafsky, D. & Martin, J.H. (2008). Speech and Language Processing. Prentice Hall, Segunda Edición.](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)\n",
    "\n",
    "[Manning, C. & Schütze, H. (1999). Foundations of Statistical Natural Language Processing, MIT Press. Cambridge, MA.](https://nlp.stanford.edu/fsnlp/)\n",
    "\n",
    "[Chen, R.,  Haitao L. & Altmann, G. (2016). Entropy in different text types. Digital Scholarship in the Humanities, vol. 32, Issue 3, pp. 528–542.](https://www.researchgate.net/publication/299485729_Entropy_in_Different_Text_Types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apéndice\n",
    "\n",
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import io\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords y símbolos\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "spanish_stopwords.extend(['si', 'mas'])\n",
    "symbols = list(set(\"«—;:,.\\\\-\\\"'/()[]¿?¡!{}~<>|\\r_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_str):\n",
    "    '''\n",
    "    Converts input string to lower case and removes accents and symbols.\n",
    "    '''\n",
    "    input_str = input_str.lower()\n",
    "    # input_str = input_str.replace('\\n', '')\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u''.join([c for c in nfkd_form if not unicodedata.combining(c) and\\\n",
    "        c not in symbols and not c.isnumeric()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para determinar entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia(input_str):\n",
    "    '''\n",
    "    Calcula la entropía de los caracteres o palabras en un texto\n",
    "    '''\n",
    "    # Contar la frecuencia de cada carácter en el texto\n",
    "    freqs = Counter(input_str)\n",
    "    # Calcular la probabilidad de cada carácter\n",
    "    probs = [float(freqs[c]) / len(input_str) for c in freqs]\n",
    "    # Calcular la entropía\n",
    "    entropy = - sum(p * math.log2(p) for p in probs)\n",
    "    return entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar la entropía global de los textos 1-5\n",
    "\n",
    "Se leen los 5 textos y se guardan en un diccionario (para poder iterarlos), eliminando\n",
    "símbolos, acentos y saltos de línea en el proceso.\n",
    "\n",
    "Posteriormente se calcula la entropía para cada uno de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dict(text_1 = [], text_2 = [], text_3 = [], text_4 = [], text_5 = [])\n",
    "\n",
    "for text in texts:\n",
    "    # Read lines from text files changing to lower and removing accents\n",
    "    raw = ''.join([preprocess(line) for line in io.open\\\n",
    "        (f'./text_files/{text}.txt', 'r', encoding = 'UTF-8').readlines()])\n",
    "\n",
    "    # Remove whitespaces and line breaks\n",
    "    texts[text] = raw.replace(' ', '').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía de text_1: 3.080115476 \tNúmero de caracteres: 4,267 \tVocales: 1,863\n",
      "Entropía de text_2: 3.087512876 \tNúmero de caracteres: 3,227 \tVocales: 1,360\n",
      "Entropía de text_3: 3.265331452 \tNúmero de caracteres: 1,326 \tVocales: 485\n",
      "Entropía de text_4: 3.116194809 \tNúmero de caracteres: 2,946 \tVocales: 1,179\n",
      "Entropía de text_5: 3.171419752 \tNúmero de caracteres: 1,340 \tVocales: 557\n"
     ]
    }
   ],
   "source": [
    "for key, value in texts.items():\n",
    "    print(f'Entropía de {key}: {entropia(value):.10}' +\n",
    "        f' \\tNúmero de caracteres: {len(value):,}' +\n",
    "        f' \\tVocales: {sum(1 for c in value if c in \"aeiou\"):,}')\n",
    "    #print(f'{100*sum(1 for c in value if c in \"aeiou\")/len(value):.4}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropía global de los libros 1 y 2\n",
    "\n",
    "Se comienza por importar los textos, removiendo acentos, símbolos y saltos de línea\n",
    "en el proceso.\n",
    "\n",
    "Posteriormente calcula la entropía para ambos textos (incluyendo stopwords).\n",
    "\n",
    "Finalmente, se remueven las stopwords en español y se repite el ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import books\n",
    "books = dict(libro_1 = [], libro_2 = [])\n",
    "\n",
    "for book in books:\n",
    "    books[book] = ''.join([preprocess(line.replace('\\n', ' ')) for line in io.open\\\n",
    "        (f'./text_files/{book}.txt', 'r', encoding = 'UTF-8').readlines()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía global a nivel palabra considerando stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía del libro_1: 9.228294320935682 \tNúmero de palabras: 56,810\n",
      "Entropía del libro_2: 9.611731124522716 \tNúmero de palabras: 50,536\n"
     ]
    }
   ],
   "source": [
    "for key, value in books.items():\n",
    "    print(f'Entropía del {key}: {entropia(value.split())}'+\n",
    "        f' \\tNúmero de palabras: {len(value.split()):,}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía global a nivel palabra sin considerar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover stopwords\n",
    "books_no_sw = dict()\n",
    "for key, value in books.items():\n",
    "    books_no_sw[key] = ' '.join([w for w in value.split() if w not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía del libro_1 sin stopwords: 11.353860554400953 \tNúmero de palabras: 26,307\n",
      "Entropía del libro_2 sin stopwords: 11.723176507369077 \tNúmero de palabras: 25,170\n"
     ]
    }
   ],
   "source": [
    "for key, value in books_no_sw.items():\n",
    "    print(f'Entropía del {key} sin stopwords: {entropia(value.split())}' +\n",
    "        f' \\tNúmero de palabras: {len(value.split()):,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7938b701c79cfa2e3008410fb6f988539f394a3c31f96e11d7a250bc3f13a15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
