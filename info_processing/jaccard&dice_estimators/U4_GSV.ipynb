{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "La tarea de medir la similitud entre textos es fundamental en varias aplicaciones\n",
    "de procesamiento de lenguaje natural, como la recuperación de información, la\n",
    "clasificación de documentos y la detección de plagio. Las medidas de similitud\n",
    "son una forma de cuantificar la similitud entre dos textos en términos de la\n",
    "similitud de sus términos. En esta tarea, se utilizan los coeficientes de Jaccard\n",
    "y Dice para obtener las similitudes entre textos.\n",
    "\n",
    "El coeficiente de Jaccard es una medida de similitud que se utiliza para comparar\n",
    "la similitud entre dos conjuntos. En el contexto del procesamiento de lenguaje\n",
    "natural, los conjuntos son los términos de dos textos. La fórmula del coeficiente\n",
    "de Jaccard se define como la división entre el número de términos que aparecen\n",
    "en ambos textos y el número total de términos que aparecen en al menos uno de\n",
    "los dos textos.\n",
    "\n",
    "$$Jaccard(X,Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Por otro lado, el coeficiente de Dice es una medida de similitud que se utiliza\n",
    "para comparar la similitud entre dos conjuntos, pero se considera más adecuado\n",
    "para comparar dos textos cortos. La fórmula del coeficiente de Dice se define\n",
    "como el doble del número de términos que aparecen en ambos textos dividido por\n",
    "la suma de los términos de ambos textos.\n",
    "\n",
    "$$Dice(X,Y) = 2\\times \\frac{|X\\cap Y|}{|X| + |Y|}$$\n",
    "\n",
    "En la siguiente tarea estudiará su aplicación en la detección de plagio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo\n",
    "\n",
    "Para llevar a cabo el expermiento se utilizaron dos grupos de textos: documentos\n",
    "fuente referenciados como *source documents* y los *suspicious-documents*, los cuales\n",
    "son una mezcla de documentos que tienen fragmentos de los documentos fuente plagiados\n",
    "y otros que simularon plagio pero no se plagió ningún párrafo de los documentos\n",
    "fuente. Se cuenta con 237 *source documents* y con 2,370 *suspicious documents*.\n",
    "\n",
    "La tarea consiste en detectar el plagio entre los *source documents* y los\n",
    "*suspicious documents* a través de los coeficientes de Jaccard y Dice. Para lograr\n",
    "este objetivo se siguió la siguiente metodología:\n",
    "\n",
    "1. Importar los textos y pre-procesarlos (cambiar texto a minúsuclas, remover símbolos\n",
    "y realizar *stemming*, es decir, normalizar el texto reduciendo las palabras a su\n",
    "raíz). Los textos se guardaron en dos diccionarios: *source_docs* y *suspicious_docs*\n",
    "para facilitar la iteracón entre ellos. (Apéndices 5.1 y 5.2)\n",
    "\n",
    "2. Definir funciones para obtener los coeficientes de Jaccard y Dice (Apéndice 5.3)\n",
    "\n",
    "3. Correr los algoritmos por cada *source document* contra cada uno de los\n",
    "*suspicious documents*. Para guardar los resultados se utilizaron diccionarios\n",
    "anidados. También se aprovechó para ordenar los resultados de manera descendiente\n",
    "con base en el promedio de los coeficientes de Jaccard y Dice. (Apéndice 5.4)\n",
    "\n",
    "El siguiente punto de la tarea consistía en obtener una muestra de 20 *source documents*\n",
    "y obtener los 3 *suspicious documents* con los coeficientes de similitud más altos\n",
    "por cada *source document*. Para este punto de la tarea se escogieron los *source documents*\n",
    "que tenían un mayor promedio de similitud (proceso en Apéndice 5.4). Se obtuvieron\n",
    "los resultados mostrados en la Tabla 1 en Apéndice 5.5, la cual muestra las parejas\n",
    "de textos (*source* vs *suspicious*) y sus respectivos coeficientes.\n",
    "\n",
    "A continuación se presentan las 3 parejas de los archivos más parecidos dentro de\n",
    "la muestra de 20 *source-documents*.\n",
    "\n",
    "| Source Document\t| Suspicious Document |\tJaccard Coef | Dice Coef |\n",
    "|---|---|---|---|\n",
    "|source-document0043.txt\t| suspicious-document0429.txt\t|0.219089\t|0.359431|\n",
    "|source-document0160.txt\t| suspicious-document1599.txt\t|0.217949\t|0.357895|\n",
    "|source-document0011.txt\t| suspicious-document0110.txt\t|0.204545\t|0.339623|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final del Apéndice, en la sección 5.6, se imprimen los dos textos con las medidas\n",
    "de similitud más altas.\n",
    "\n",
    "Tras haberlos leído con detenimiento, podemos ver que ambos textos contienen el siguiente fragmento:\n",
    "\n",
    "*\"In 1960 Hurricane Donna struck the Florida Keys at Marathon, then raked across Naples and Fort Myers before weakening inland. Last season, Atlantic hurricanes killed 505 people. Gilbert killed more than 300 and did heavy damage in Mexico, Jamaica, Haiti and the Dominican Republic as it blasted across the western Caribbean and part of the Gulf of Mexico, including the Florida Keys, the Florida Straits and Cuba. Joan hovered off the coast of Central America for days before howling in with top winds of 135 mph. Joan caused mudslides, floods and other damage in Nicaragua, Costa Rica, Colombia and Panama.\"*\n",
    "\n",
    "debido a esto se puede inferir porqué ambos coeficientes han salido altos, permitiéndo\n",
    "identificar el plagio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "En conclusión, en este trabajo se aplicaron las medidas de similitud de Jaccard y Dice para evaluar la similitud entre los textos de los grupos source-files y suspicious-files. Se encontró que ambas medidas proporcionaron resultados similares en cuanto a la similitud entre los documentos, lo que indica que ambas son útiles para este tipo de análisis. Se identificaron varios pares de documentos con similitudes destacables en ambos grupos, lo que sugiere que estos documentos pueden haber sido copiados o influenciados entre sí. En general, el uso de medidas de similitud puede ser una herramienta valiosa en la identificación de plagio y la evaluación de la originalidad del contenido textual."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "[Tversky, A., & Gati, I. (1978). Studies of similarity. Cognition and categorization, 1 (1978), 79-98.](https://pages.ucsd.edu/~scoulson/203/tvgati.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apéndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir funciones para el preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "symbols = list(set(\"«—;:,.\\\\-\\\"'/()[]¿?¡!{}~<>|\\r_'\\n'`\"))\n",
    "\n",
    "def preprocess(input_str):\n",
    "    '''\n",
    "    Returns a given string in lower case, without symbols and accents. It also\n",
    "    removes stopwords and does stemming.\n",
    "    '''\n",
    "\n",
    "    # Convert to lower case\n",
    "    input_str = input_str.lower()\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "\n",
    "    # Remove symbols and accents\n",
    "    plain_text = ''.join([c for c in nfkd_form if not unicodedata.combining(c) \\\n",
    "                          and c not in symbols])\n",
    "\n",
    "    # Do stemming and remove stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    return u' '.join([stemmer.stem(word) for word in plain_text.split() if word\\\n",
    "                      not in english_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import source documents\n",
    "\n",
    "folder_path = \"source-documents\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "source_docs = {}\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file), \"r\") as f:\n",
    "            text = f.read()\n",
    "            text = preprocess(text) # Data preprocessing\n",
    "            source_docs[file] = text\n",
    "\n",
    "\n",
    "# Import suspicious documents\n",
    "\n",
    "folder_path = \"suspicious-documents\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "suspicious_docs = {}\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file), \"r\") as f:\n",
    "            text = f.read()\n",
    "            text = preprocess(text) # Data preprocessing\n",
    "            suspicious_docs[file] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir algoritmos de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard\n",
    "\n",
    "def jaccard(text1, text2):\n",
    "    '''\n",
    "    Calculates Jaccard's similarity coeficient given a pair of texts.\n",
    "    The higher the coeficient the bigger the similarity\n",
    "    '''\n",
    "    # Create sets\n",
    "    set1 = set(text1.split())\n",
    "    set2 = set(text2.split())\n",
    "\n",
    "    # Calculate union and instersection\n",
    "    union = len(set1.union(set2))\n",
    "    intersection = len(set1.intersection(set2))\n",
    "\n",
    "    # Calculate Jaccard's similarity coeficient\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# Dice\n",
    "\n",
    "def dice(text1, text2):\n",
    "    '''\n",
    "    Calculates Dice's similarity coeficient given a pair of texts.\n",
    "    The higher the coeficient the bigger the similarity\n",
    "    '''\n",
    "    # Create sets\n",
    "    set1 = set(text1.split())\n",
    "    set2 = set(text2.split())\n",
    "\n",
    "    # Calculate instersection\n",
    "    intersection = len(set1.intersection(set2))\n",
    "\n",
    "    # Return Dice's similarity coeficient\n",
    "    return 2 * intersection / (len(set1) + len(set2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los resultados\n",
    "\n",
    "results = {}\n",
    "\n",
    "for so_doc, so_txt in source_docs.items():\n",
    "    results[so_doc] = {}\n",
    "    for sus_doc, sus_txt in suspicious_docs.items():\n",
    "        results[so_doc][sus_doc] = [jaccard(so_txt, sus_txt), dice(so_txt, sus_txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results\n",
    "def avg(lst):\n",
    "    return sum(lst)/len(lst)\n",
    "\n",
    "# Function to get the average similarity coeficient of the \n",
    "def avg_dict(d):\n",
    "    means = []\n",
    "    for lst in list(d.values())[:5]:\n",
    "        means.append(sum(lst)/len(lst))\n",
    "    return sum(means)/len(means)\n",
    "\n",
    "# Sort inner dictionary (comparisons) based on the average between the two\n",
    "# coeficients\n",
    "for so_doc in results:\n",
    "    results[so_doc] = dict(sorted(results[so_doc].items(), \\\n",
    "                                  key = lambda x: avg(x[1]), reverse=True))\n",
    "\n",
    "# Get a list of the source-documents sorted based on the one that averages more\n",
    "# higher coeficients\n",
    "sorted_docs = sorted(results, key = lambda x: avg_dict(results[x]), reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imprimir resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Source Document | Suspicious Document | Jaccard Coef | Dice Coef |\n",
      "| --- | --- | --- | --- |\n",
      "| source-document0016.txt | suspicious-document0245.txt | 0.1797 | 0.3046 |\n",
      "| source-document0016.txt | suspicious-document0177.txt | 0.1794 | 0.3042 |\n",
      "| source-document0016.txt | suspicious-document0183.txt | 0.1793 | 0.3041 |\n",
      "| source-document0040.txt | suspicious-document0204.txt | 0.1793 | 0.3041 |\n",
      "| source-document0040.txt | suspicious-document1465.txt | 0.1783 | 0.3026 |\n",
      "| source-document0040.txt | suspicious-document0225.txt | 0.178 | 0.3022 |\n",
      "| source-document0073.txt | suspicious-document0146.txt | 0.1784 | 0.3028 |\n",
      "| source-document0073.txt | suspicious-document2044.txt | 0.1784 | 0.3027 |\n",
      "| source-document0073.txt | suspicious-document2067.txt | 0.1782 | 0.3025 |\n",
      "| source-document0021.txt | suspicious-document2198.txt | 0.2044 | 0.3394 |\n",
      "| source-document0021.txt | suspicious-document0759.txt | 0.1694 | 0.2896 |\n",
      "| source-document0021.txt | suspicious-document0722.txt | 0.1666 | 0.2856 |\n",
      "| source-document0130.txt | suspicious-document2040.txt | 0.1782 | 0.3025 |\n",
      "| source-document0130.txt | suspicious-document2079.txt | 0.1748 | 0.2976 |\n",
      "| source-document0130.txt | suspicious-document1418.txt | 0.174 | 0.2965 |\n",
      "| source-document0060.txt | suspicious-document0183.txt | 0.1744 | 0.297 |\n",
      "| source-document0060.txt | suspicious-document0245.txt | 0.1735 | 0.2957 |\n",
      "| source-document0060.txt | suspicious-document0194.txt | 0.1733 | 0.2954 |\n",
      "| source-document0158.txt | suspicious-document0225.txt | 0.1697 | 0.2902 |\n",
      "| source-document0158.txt | suspicious-document0183.txt | 0.1679 | 0.2875 |\n",
      "| source-document0158.txt | suspicious-document0177.txt | 0.1664 | 0.2854 |\n",
      "| source-document0023.txt | suspicious-document0225.txt | 0.1709 | 0.2919 |\n",
      "| source-document0023.txt | suspicious-document0183.txt | 0.1674 | 0.2867 |\n",
      "| source-document0023.txt | suspicious-document0245.txt | 0.1662 | 0.285 |\n",
      "| source-document0152.txt | suspicious-document1517.txt | 0.1851 | 0.3124 |\n",
      "| source-document0152.txt | suspicious-document1734.txt | 0.1621 | 0.279 |\n",
      "| source-document0152.txt | suspicious-document1543.txt | 0.1619 | 0.2787 |\n",
      "| source-document0001.txt | suspicious-document0010.txt | 0.1736 | 0.2958 |\n",
      "| source-document0001.txt | suspicious-document2205.txt | 0.1624 | 0.2794 |\n",
      "| source-document0001.txt | suspicious-document2048.txt | 0.1613 | 0.2778 |\n",
      "| source-document0014.txt | suspicious-document0139.txt | 0.1786 | 0.3031 |\n",
      "| source-document0014.txt | suspicious-document0140.txt | 0.1727 | 0.2946 |\n",
      "| source-document0014.txt | suspicious-document2018.txt | 0.1543 | 0.2674 |\n",
      "| source-document0104.txt | suspicious-document1040.txt | 0.177 | 0.3008 |\n",
      "| source-document0104.txt | suspicious-document2198.txt | 0.1591 | 0.2746 |\n",
      "| source-document0104.txt | suspicious-document2240.txt | 0.1585 | 0.2736 |\n",
      "| source-document0160.txt | suspicious-document1599.txt | 0.2179 | 0.3579 |\n",
      "| source-document0160.txt | suspicious-document1600.txt | 0.1991 | 0.332 |\n",
      "| source-document0160.txt | suspicious-document1598.txt | 0.1671 | 0.2863 |\n",
      "| source-document0050.txt | suspicious-document0713.txt | 0.1632 | 0.2806 |\n",
      "| source-document0050.txt | suspicious-document0499.txt | 0.1618 | 0.2785 |\n",
      "| source-document0050.txt | suspicious-document2198.txt | 0.1605 | 0.2766 |\n",
      "| source-document0204.txt | suspicious-document2040.txt | 0.202 | 0.3362 |\n",
      "| source-document0204.txt | suspicious-document2039.txt | 0.1897 | 0.3189 |\n",
      "| source-document0204.txt | suspicious-document2079.txt | 0.1453 | 0.2538 |\n",
      "| source-document0011.txt | suspicious-document0110.txt | 0.2045 | 0.3396 |\n",
      "| source-document0011.txt | suspicious-document0108.txt | 0.1544 | 0.2674 |\n",
      "| source-document0011.txt | suspicious-document0109.txt | 0.1476 | 0.2572 |\n",
      "| source-document0036.txt | suspicious-document0891.txt | 0.1625 | 0.2795 |\n",
      "| source-document0036.txt | suspicious-document0942.txt | 0.1592 | 0.2746 |\n",
      "| source-document0036.txt | suspicious-document0788.txt | 0.1585 | 0.2736 |\n",
      "| source-document0043.txt | suspicious-document0429.txt | 0.2191 | 0.3594 |\n",
      "| source-document0043.txt | suspicious-document0430.txt | 0.1626 | 0.2796 |\n",
      "| source-document0043.txt | suspicious-document2040.txt | 0.139 | 0.2441 |\n",
      "| source-document0046.txt | suspicious-document0459.txt | 0.1951 | 0.3265 |\n",
      "| source-document0046.txt | suspicious-document0460.txt | 0.1818 | 0.3077 |\n",
      "| source-document0046.txt | suspicious-document1963.txt | 0.1386 | 0.2435 |\n",
      "| source-document0142.txt | suspicious-document2040.txt | 0.1736 | 0.2959 |\n",
      "| source-document0142.txt | suspicious-document1419.txt | 0.1706 | 0.2915 |\n",
      "| source-document0142.txt | suspicious-document1418.txt | 0.154 | 0.267 |\n"
     ]
    }
   ],
   "source": [
    "# Código para obtener la tabla en formato markdown\n",
    "counter = 0\n",
    "print('| Source Document | Suspicious Document | Jaccard Coef | Dice Coef |')\n",
    "print('| --- | --- | --- | --- |')\n",
    "#for so_doc in results:\n",
    "for so_doc in sorted_docs:\n",
    "    keys = list(results[so_doc].keys())\n",
    "    counter += 1\n",
    "    if counter > 20:\n",
    "        break\n",
    "    print(f'| {so_doc} | {keys[0]} | {results[so_doc][keys[0]][0]:.4} | {results[so_doc][keys[0]][1]:.4} |')\n",
    "    print(f'| {so_doc} | {keys[1]} | {results[so_doc][keys[1]][0]:.4} | {results[so_doc][keys[1]][1]:.4} |')\n",
    "    print(f'| {so_doc} | {keys[2]} | {results[so_doc][keys[2]][0]:.4} | {results[so_doc][keys[2]][1]:.4} |')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla 1\n",
    "\n",
    "| Source Document | Suspicious Document | Jaccard Coef | Dice Coef |\n",
    "| --- | --- | --- | --- |\n",
    "| source-document0016.txt | suspicious-document0245.txt | 0.1797 | 0.3046 |\n",
    "| source-document0016.txt | suspicious-document0177.txt | 0.1794 | 0.3042 |\n",
    "| source-document0016.txt | suspicious-document0183.txt | 0.1793 | 0.3041 |\n",
    "| source-document0040.txt | suspicious-document0204.txt | 0.1793 | 0.3041 |\n",
    "| source-document0040.txt | suspicious-document1465.txt | 0.1783 | 0.3026 |\n",
    "| source-document0040.txt | suspicious-document0225.txt | 0.178 | 0.3022 |\n",
    "| source-document0073.txt | suspicious-document0146.txt | 0.1784 | 0.3028 |\n",
    "| source-document0073.txt | suspicious-document2044.txt | 0.1784 | 0.3027 |\n",
    "| source-document0073.txt | suspicious-document2067.txt | 0.1782 | 0.3025 |\n",
    "| source-document0021.txt | suspicious-document2198.txt | 0.2044 | 0.3394 |\n",
    "| source-document0021.txt | suspicious-document0759.txt | 0.1694 | 0.2896 |\n",
    "| source-document0021.txt | suspicious-document0722.txt | 0.1666 | 0.2856 |\n",
    "| source-document0130.txt | suspicious-document2040.txt | 0.1782 | 0.3025 |\n",
    "| source-document0130.txt | suspicious-document2079.txt | 0.1748 | 0.2976 |\n",
    "| source-document0130.txt | suspicious-document1418.txt | 0.174 | 0.2965 |\n",
    "| source-document0060.txt | suspicious-document0183.txt | 0.1744 | 0.297 |\n",
    "| source-document0060.txt | suspicious-document0245.txt | 0.1735 | 0.2957 |\n",
    "| source-document0060.txt | suspicious-document0194.txt | 0.1733 | 0.2954 |\n",
    "| source-document0158.txt | suspicious-document0225.txt | 0.1697 | 0.2902 |\n",
    "| source-document0158.txt | suspicious-document0183.txt | 0.1679 | 0.2875 |\n",
    "| source-document0158.txt | suspicious-document0177.txt | 0.1664 | 0.2854 |\n",
    "| source-document0023.txt | suspicious-document0225.txt | 0.1709 | 0.2919 |\n",
    "| source-document0023.txt | suspicious-document0183.txt | 0.1674 | 0.2867 |\n",
    "| source-document0023.txt | suspicious-document0245.txt | 0.1662 | 0.285 |\n",
    "| source-document0152.txt | suspicious-document1517.txt | 0.1851 | 0.3124 |\n",
    "| source-document0152.txt | suspicious-document1734.txt | 0.1621 | 0.279 |\n",
    "| source-document0152.txt | suspicious-document1543.txt | 0.1619 | 0.2787 |\n",
    "| source-document0001.txt | suspicious-document0010.txt | 0.1736 | 0.2958 |\n",
    "| source-document0001.txt | suspicious-document2205.txt | 0.1624 | 0.2794 |\n",
    "| source-document0001.txt | suspicious-document2048.txt | 0.1613 | 0.2778 |\n",
    "| source-document0014.txt | suspicious-document0139.txt | 0.1786 | 0.3031 |\n",
    "| source-document0014.txt | suspicious-document0140.txt | 0.1727 | 0.2946 |\n",
    "| source-document0014.txt | suspicious-document2018.txt | 0.1543 | 0.2674 |\n",
    "| source-document0104.txt | suspicious-document1040.txt | 0.177 | 0.3008 |\n",
    "| source-document0104.txt | suspicious-document2198.txt | 0.1591 | 0.2746 |\n",
    "| source-document0104.txt | suspicious-document2240.txt | 0.1585 | 0.2736 |\n",
    "| source-document0160.txt | suspicious-document1599.txt | 0.2179 | 0.3579 |\n",
    "| source-document0160.txt | suspicious-document1600.txt | 0.1991 | 0.332 |\n",
    "| source-document0160.txt | suspicious-document1598.txt | 0.1671 | 0.2863 |\n",
    "| source-document0050.txt | suspicious-document0713.txt | 0.1632 | 0.2806 |\n",
    "| source-document0050.txt | suspicious-document0499.txt | 0.1618 | 0.2785 |\n",
    "| source-document0050.txt | suspicious-document2198.txt | 0.1605 | 0.2766 |\n",
    "| source-document0204.txt | suspicious-document2040.txt | 0.202 | 0.3362 |\n",
    "| source-document0204.txt | suspicious-document2039.txt | 0.1897 | 0.3189 |\n",
    "| source-document0204.txt | suspicious-document2079.txt | 0.1453 | 0.2538 |\n",
    "| source-document0011.txt | suspicious-document0110.txt | 0.2045 | 0.3396 |\n",
    "| source-document0011.txt | suspicious-document0108.txt | 0.1544 | 0.2674 |\n",
    "| source-document0011.txt | suspicious-document0109.txt | 0.1476 | 0.2572 |\n",
    "| source-document0036.txt | suspicious-document0891.txt | 0.1625 | 0.2795 |\n",
    "| source-document0036.txt | suspicious-document0942.txt | 0.1592 | 0.2746 |\n",
    "| source-document0036.txt | suspicious-document0788.txt | 0.1585 | 0.2736 |\n",
    "| source-document0043.txt | suspicious-document0429.txt | 0.2191 | 0.3594 |\n",
    "| source-document0043.txt | suspicious-document0430.txt | 0.1626 | 0.2796 |\n",
    "| source-document0043.txt | suspicious-document2040.txt | 0.139 | 0.2441 |\n",
    "| source-document0046.txt | suspicious-document0459.txt | 0.1951 | 0.3265 |\n",
    "| source-document0046.txt | suspicious-document0460.txt | 0.1818 | 0.3077 |\n",
    "| source-document0046.txt | suspicious-document1963.txt | 0.1386 | 0.2435 |\n",
    "| source-document0142.txt | suspicious-document2040.txt | 0.1736 | 0.2959 |\n",
    "| source-document0142.txt | suspicious-document1419.txt | 0.1706 | 0.2915 |\n",
    "| source-document0142.txt | suspicious-document1418.txt | 0.154 | 0.267 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "counter = 0\n",
    "for so_doc in sorted_docs:\n",
    "    keys = list(results[so_doc].keys())\n",
    "    counter += 1\n",
    "    if counter > 20:\n",
    "        break\n",
    "    row1 = [so_doc, keys[0], results[so_doc][keys[0]][0], results[so_doc][keys[0]][1]]\n",
    "    row2 = [so_doc, keys[1], results[so_doc][keys[1]][0], results[so_doc][keys[1]][1]]\n",
    "    row3 = [so_doc, keys[2], results[so_doc][keys[2]][0], results[so_doc][keys[2]][1]]\n",
    "    data.extend([row1, row2, row3])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Source Document', 'Suspicious Document', 'Jaccard Coef', 'Dice Coef'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos con mayor similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Document</th>\n",
       "      <th>Suspicious Document</th>\n",
       "      <th>Jaccard Coef</th>\n",
       "      <th>Dice Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>source-document0043.txt</td>\n",
       "      <td>suspicious-document0429.txt</td>\n",
       "      <td>0.219089</td>\n",
       "      <td>0.359431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>source-document0160.txt</td>\n",
       "      <td>suspicious-document1599.txt</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.357895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>source-document0011.txt</td>\n",
       "      <td>suspicious-document0110.txt</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.339623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Source Document          Suspicious Document  Jaccard Coef  \\\n",
       "51  source-document0043.txt  suspicious-document0429.txt      0.219089   \n",
       "36  source-document0160.txt  suspicious-document1599.txt      0.217949   \n",
       "45  source-document0011.txt  suspicious-document0110.txt      0.204545   \n",
       "\n",
       "    Dice Coef  \n",
       "51   0.359431  \n",
       "36   0.357895  \n",
       "45   0.339623  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = df.sort_values(by='Jaccard Coef', ascending=False).head(3)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasters preparing for Thursday's opening of the Atlantic hurricane season wish they could predict the arrival of new technological help they say may be crucial to ever-growing coastal populations. The Air Force has agreed to fly hurricane reconnaissance flights for two more years, but has made it clear it plans to phase out the missions. And only one satellite is available for tracking hurricanes. \"We just have nothing right now to lean on,\" says Ken McKinnon, a spokesman for U.S. Rep. Tom Lewis of North Palm Beach, Fla., who has introduced a bill in Congress to keep hurricane hunters flying at least another five years. \"We've got one satellite and they're telling us it'll do the job. If it blinks, how do you track weather?\" The Air Force doesn't want to be involved. \"We have in the last few years examined our need for manned weather reconnaissance and feel there's no real compelling military reason,\" said spokesman Lt. Col. Darrell Hayes. \"We're not disputing that the hurricane center and the weather service need the data. We're just saying there may be more appropriate agencies to provide the information,\" he said, adding that the service had approached the National Oceanic and Atmospheric Administration about taking over the flights. Besides the flights, forecasters depend on radar and satellite data. The single working weather satellite wasn't intended to be alone. A second satellite failed, and a replacement for the failed craft was blown up in a mishap on the launch pad, forcing forecasters to make do. There are new satellites on the horizon, says Bob Sheets, director of the National Hurricane Center. But they've been due for a long time and aren't expected before late 1990. \"It is a major concern for us,\" Sheets said. Forecasters also are worried about a shift in the pattern of hurricane activity in recent years. Since 1985, Sheets said, there seem to be more hurricanes and they're more likely to hit the United States. \"We may be in an upswing,\" he said, \"possibly back to the pattern of the '40s, '50s and '60s when we had a tremendous number of landfall hurricanes.\" Max Mayfield, hurricane specialist at the National Weather Service in Miami, said experts don't known enough yet about hurricanes to tell if this is just a peak in activity, or a return to the 50s and 60s. \"Now we can see past the Antilles out into the Atlantic, and over toward Hawaii on the west,\" said forecaster Hal Gerrish. \"We'd like to be able to see all the way to Africa,\" which is where Atlantic hurricanes develop, he said. The need for improved tracking systems is important because more and more people are moving to coastal locations likely to be affected by storms. \"I spoke to about 5,000 people on the west coast of Florida,\" Sheets said. \"Ninety-plus percent of them were from the Midwest or Northeast and had just come to Florida. They really have very little concept of what a hurricane is.\" During the average Atlantic hurricane season, which stretches from June through the end of November, six tropical storms will grow into hurricanes, with heavy rains and winds of 74 mph. Donna, in 1960, struck the Florida Keys at Marathon, then raked across Naples and Fort Myers before weakening inland. Last season, 505 people died in Atlantic hurricanes, including Gilbert and Joan. Gilbert killed more than 300 people and did heavy damage in Mexico, Jamaica, Haiti and the Dominican Republic as it blasted across the the western Caribbean and part of the Gulf of Mexico _ including the Florida Keys, the Florida Straits and Cuba. Joan hovered off the coast of Central America for days before howling in with top winds of 135 mph. The storm caused mudslides, floods and other damage in Nicaragua, Costa Rica, Colombia and Panama.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('source-documents/source-document0043.txt', \"r\") as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TROY,  Mich.  ( AP )--  Delphi Automotive Systems Corp. , the auto-parts manufacturer soon to be independent from  General Motors Corp. , has no more money-losing plants, is getting cooperation from its unions to cut costs and is winning more non-GM business, its chairman said  Monday . As the world's largest parts-maker,  Delphi  also plans to be a major player in the industry's consolidation through an aggressive acquisition drive,  J.T. Battenberg III  told reporters before departing on a worldwide roadshow to raise his company's profile among investors.  Delphi  was once a disparate collection of parts operations that, with parent  GM , was near bankruptcy in the  early 1990s . Though it lost  $ 93 million  last year because of several one-time costs,  Delphi  earned  $ 284 million  in the  first quarter this year .  GM  is cutting  Delphi  loose to focus on its core business : building cars and trucks.  Delphi  executives say they expect their business to grow as other automakers no longer have to fear working with a supplier owned by their biggest competitor.\n",
      "There's evidence that's already happening, even though the spinoff won't be completed until 1960. In the  first quarter , Fort Myers won  $ 4 billion  in new contracts with  GM  and a surprising  $ 2 billion  worth of non-GM contracts.  Delphi  stock price increased  18 percent  in its first three months. \" The stock has performed well,\" said analyst Donna of Bear,  Stearns & Co.  \" They're certainly winning business, and that's picked up since their announcement of the spinoff.\"  Delphi , based in Florida Keys, Naples, and  Battenberg  will face their first big test come  summer  when they will work out details of a new contract with the company's largest union, the  United Auto Workers . Talks already are under way with some  UAW  locals and  Battenberg  said there has been progress.\n",
      "\n",
      "Air Force hit Congress with two strikes last Thursday that shut down Air Force's North American assembly plants and cost National Oceanic and Atmospheric Administration  $ 450 million . Both companies are trying to repair their long-contentious relationship with the union. Battenberg declined to comment in detail on that relationship but said he was in \" personal touch\" with National Hurricane Center leaders. Though company insiders say National Weather Service president Ken McKinnon has been cooperative, publicly he has criticized the spinoff and urged Northeast to retain Ninety-plus percent of the company. The Delphi-UAW talks will coincide with the union's triennial contract negotiations with Fort Myers,  Ford Motor Co.  and the  Chrysler  unit of  DaimlerChrysler AG . The  UAW  is expected to demand that  Delphi 's hourly workers get virtually the same deal as  GM 's hourly workers.\n",
      "\n",
      "Delphi no longer has any plants that are unprofitable, in some cases because its unions agreed to relax restrictive work rules,  Battenberg  said. In  Kokomo ,  Ind. , for example, the  UAW  agreed to work rule changes to allow the electronics plant to operate 24 hours a day, seven days a week.  Battenberg  said  Delphi  plans to focus on acquiring companies that can supply future technology, especially in the area of high-tech electronics as computers and satellite telecommunications become more integrated into the design of car and truck interiors. \" I look at  Delphi  becoming an electronics company that makes products for vehicles, which is a lot more attractive than a traditional auto-parts company,\"  Lawrence  said. Though  Delphi  has been trimming its work force through attrition, the company may end up adding workers if it meets its goals to increase new business,  Battenberg  said. Later this month,  Delphi  will debut a  $ 1 million  TV-and-print advertising campaign to coincide with the  Indianapolis  500 auto race.\n",
      "In 1960 Hurricane Donna struck the Florida Keys at Marathon, then raked across Naples and Fort Myers before weakening inland. Last season, Atlantic hurricanes killed 505 people. Gilbert killed more than 300 and did heavy damage in Mexico, Jamaica, Haiti and the Dominican Republic as it blasted across the western Caribbean and part of the Gulf of Mexico, including the Florida Keys, the Florida Straits and Cuba. Joan hovered off the coast of Central America for days before howling in with top winds of 135 mph. Joan caused mudslides, floods and other damage in Nicaragua, Costa Rica, Colombia and Panama.\n",
      "\n",
      " The campaign and 20-city roadshow are intended to make  Delphi  a brand known outside the auto industry. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('suspicious-documents/suspicious-document0429.txt', \"r\") as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
